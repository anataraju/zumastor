--- 2.6.24.3.base/drivers/md/dm.c	2008-03-16 03:46:00.000000000 -0700
+++ 2.6.24.3/drivers/md/dm.c	2008-03-16 04:06:34.000000000 -0700
@@ -562,8 +562,7 @@ static sector_t max_io_len(struct mapped
 	return len;
 }
 
-static void __map_bio(struct dm_target *ti, struct bio *clone,
-		      struct dm_target_io *tio)
+static void __map_bio(struct dm_target *ti, struct bio *clone, struct dm_target_io *tio)
 {
 	int r;
 	sector_t sector;
@@ -609,6 +608,37 @@ static void __map_bio(struct dm_target *
 	}
 }
 
+static void dm_map(struct bio *bio, struct dm_target *ti, struct dm_target_io *tio)
+{
+	int r;
+	sector_t sector;
+
+	/*
+	 * Map the bio.  If r == 0 we don't need to do
+	 * anything, the target has assumed ownership of
+	 * this io.
+	 */
+	atomic_inc(&tio->io->io_count);
+	sector = bio->bi_sector;
+	r = ti->type->map(ti, bio, &tio->info);
+	if (r == DM_MAPIO_REMAPPED) {
+		/* the bio has been remapped so dispatch it */
+
+		blk_add_trace_remap(bdev_get_queue(bio->bi_bdev), bio,
+				    tio->io->bio->bi_bdev->bd_dev,
+				    bio->bi_sector, sector);
+
+		generic_make_request(bio);
+	} else if (r < 0 || r == DM_MAPIO_REQUEUE) {
+		/* error the io and bail out, or requeue it if needed */
+		bio_pop(bio);
+		dec_pending(tio->io, r);
+	} else if (r) {
+		DMWARN("unimplemented target map return value: %d", r);
+		BUG();
+	}
+}
+
 struct clone_info {
 	struct mapped_device *md;
 	struct dm_table *map;
@@ -685,23 +715,18 @@ static int __clone_and_map(struct clone_
 
 	max = max_io_len(ci->md, ci->sector, ti);
 
-	/*
-	 * Allocate a target io object.
-	 */
-	tio = alloc_tio(ci->md);
-	tio->io = ci->io;
-	tio->ti = ti;
-	memset(&tio->info, 0, sizeof(tio->info));
-
 	if (ci->sector_count <= max) {
 		/*
 		 * Optimise for the simple case where we can do all of
 		 * the remaining io with a single clone.
 		 */
-		clone = clone_bio(bio, ci->sector, ci->idx,
-				  bio->bi_vcnt - ci->idx, ci->sector_count,
-				  ci->md->bs);
-		__map_bio(ti, clone, tio);
+		tio = bio_push(bio, sizeof(*tio), dm_endio_pop);
+		*tio = (typeof(*tio)){ .io = ci->io, .ti = ti };
+		bio->bi_sector = ci->sector;
+		bio->bi_idx = ci->idx;
+		bio->bi_size = to_bytes(ci->sector_count);
+		bio->bi_flags &= ~(1 << BIO_SEG_VALID);
+		dm_map(bio, ti, tio);
 		ci->sector_count = 0;
 
 	} else if (to_sector(bio->bi_io_vec[ci->idx].bv_len) <= max) {
@@ -713,6 +738,9 @@ static int __clone_and_map(struct clone_
 		sector_t remaining = max;
 		sector_t bv_len;
 
+		tio = alloc_tio(ci->md);
+		*tio = (typeof(*tio)){ .io = ci->io, .ti = ti };
+
 		for (i = ci->idx; remaining && (i < bio->bi_vcnt); i++) {
 			bv_len = to_sector(bio->bi_io_vec[i].bv_len);
 
@@ -739,6 +767,9 @@ static int __clone_and_map(struct clone_
 		sector_t remaining = to_sector(bv->bv_len);
 		unsigned int offset = 0;
 
+		tio = alloc_tio(ci->md);
+		*tio = (typeof(*tio)){ .io = ci->io, .ti = ti };
+
 		do {
 			if (offset) {
 				ti = dm_table_find_target(ci->map, ci->sector);
--- 2.6.24.3.base/fs/bio.c	2008-03-16 03:46:00.000000000 -0700
+++ 2.6.24.3/fs/bio.c	2008-03-16 03:44:03.000000000 -0700
@@ -31,6 +31,63 @@
 #define BIO_POOL_SIZE 2
 #define BIO_NR_POOLS 6
 
+/* Bio push/pop (C) 2008, Daniel Phillips <phillips@phunq.net> */
+
+#define BIOSTACK_ALIGN (1 << 2)
+#define BIOCHUNK_SIZE (1 << 7)
+#define BIOCHUNK_MAGIC (0xddc0ffee)
+#define BIOCHUNK_OVERHEAD (sizeof(struct biochunk) + sizeof(struct bioframe))
+#define BIOCHUNK_PAYLOAD (BIOCHUNK_SIZE - BIOCHUNK_OVERHEAD)
+
+struct biochunk { u32 magic; void *oldstack; char frames[]; };
+
+static struct kmem_cache *biospace __read_mostly;
+
+static struct biochunk *alloc_biochunk(void)
+{
+	return kmem_cache_alloc(biospace, __GFP_NOFAIL);
+}
+
+static void free_biochunk(struct biochunk *chunk)
+{
+	kmem_cache_free(biospace, chunk);
+}
+
+void *bio_push(struct bio *bio, unsigned size, bio_end_io_t *endio)
+{
+	struct bioframe *frame = bio->bi_stack;
+	size += sizeof(struct bioframe);
+	size += -size & (BIOSTACK_ALIGN - 1);
+	if (unlikely(size > frame->stacksize)) {
+		struct biochunk *chunk = alloc_biochunk();
+		bio_end_io_t *old = frame->endio;
+		BUG_ON(size > BIOCHUNK_PAYLOAD);
+		*chunk = (struct biochunk){ .oldstack = bio->bi_stack, .magic = BIOCHUNK_MAGIC };
+		frame = bio->bi_stack = chunk->frames;
+		*frame = (struct bioframe){ .stacksize = BIOCHUNK_PAYLOAD, .endio = old };
+	}
+	bio->bi_stack += size;
+	*(struct bioframe *)bio->bi_stack = (struct bioframe){
+		.stacksize = frame->stacksize - size,
+		.framesize = size, .endio = endio };
+	return frame->space;
+}
+EXPORT_SYMBOL_GPL(bio_push);
+
+void *bio_pop(struct bio *bio)
+{
+	struct bioframe *frame = bio->bi_stack;
+	if (unlikely(!frame->framesize)) {
+		struct biochunk *chunk = bio->bi_stack - sizeof(struct biochunk);
+		BUG_ON(chunk->magic != BIOCHUNK_MAGIC);
+		frame = bio->bi_stack = chunk->oldstack;
+		free_biochunk(chunk);
+	}
+	frame = bio->bi_stack -= frame->framesize;
+	return frame->space;
+}
+EXPORT_SYMBOL_GPL(bio_pop);
+
 /*
  * a small number of entries is fine, not going to be performance critical.
  * basically we just need to survive
@@ -76,6 +133,8 @@ void bio_free(struct bio *bio, struct bi
 	const int pool_idx = BIO_POOL_IDX(bio);
 	BIO_BUG_ON(pool_idx >= BIO_NR_POOLS);
 	BIO_BUG_ON(!bio->bi_io_vec);
+	if (!((struct bioframe *)bio->bi_stack)->framesize)
+		bio_pop(bio);
 	mempool_free(bio, bio_set->bio_pools[pool_idx]);
 }
 
@@ -130,6 +189,8 @@ struct bio *bio_alloc_bioset(gfp_t gfp_m
 	bio->bi_flags |= idx << BIO_POOL_OFFSET;
 	bio->bi_max_vecs = bio_slabs[idx].nr_vecs;
 	bio->bi_io_vec = (void *)bio + sizeof(struct bio);
+	bio->bi_stack = &bio->space;
+	bio->space.framesize = sizeof(struct bioframe);
 	return bio;
 }
 
@@ -1102,6 +1163,8 @@ static int __init init_bio(void)
 	if (!bio_split_pool)
 		panic("bio: can't create split pool\n");
 
+	biospace = kmem_cache_create("biospace", BIOCHUNK_SIZE, 0, SLAB_PANIC, NULL);
+
 	return 0;
 }
 
--- 2.6.24.3.base/include/linux/bio.h	2008-03-16 03:46:00.000000000 -0700
+++ 2.6.24.3/include/linux/bio.h	2008-03-16 03:44:51.000000000 -0700
@@ -68,6 +68,11 @@ typedef void (bio_end_io_t) (struct bio 
 typedef void (bio_destructor_t) (struct bio *);
 
 /*
+ * Support endio handler stacking with per-handler private workspace
+ */
+struct bioframe { u16 framesize, stacksize; bio_end_io_t *endio; char space[]; };
+
+/*
  * main unit of I/O for the block layer and lower layers (ie drivers and
  * stacking drivers)
  */
@@ -114,6 +119,8 @@ struct bio {
 	void			*bi_private;
 
 	bio_destructor_t	*bi_destructor;	/* destructor */
+	void			*bi_stack;	/* endio stacking */
+	struct bioframe		space;
 };
 
 /*
@@ -201,12 +208,12 @@ static inline void *bio_data(struct bio 
 
 static inline void bio_set_endio(struct bio *bio, bio_end_io_t *endio)
 {
-	bio->bi_endio = endio;
+	((struct bioframe *)bio->bi_stack)->endio = endio;
 }
 
 static inline bio_end_io_t *bio_get_endio(struct bio *bio)
 {
-	return bio->bi_endio;
+	return ((struct bioframe *)bio->bi_stack)->endio;
 }
 
 /*
@@ -310,7 +317,8 @@ extern struct bio *bio_alloc(gfp_t, int)
 extern struct bio *bio_alloc_bioset(gfp_t, int, struct bio_set *);
 extern void bio_put(struct bio *);
 extern void bio_free(struct bio *, struct bio_set *);
-
+extern void *bio_push(struct bio *bio, unsigned size, bio_end_io_t *endio);
+extern void *bio_pop(struct bio *bio);
 extern void bio_endio(struct bio *, int);
 struct request_queue;
 extern int bio_phys_segments(struct request_queue *, struct bio *);
